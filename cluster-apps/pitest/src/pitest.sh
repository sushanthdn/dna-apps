#!/bin/bash
# pitest 0.0.1
# Generated by dx-app-wizard.
#
# Basic execution pattern: Your app will run on a single machine from
# beginning to end.
#
# Your job's input variables (if any) will be loaded as environment
# variables before this script runs.  Any array inputs will be loaded
# as bash arrays.
#
# Any code outside of main() (or any entry point you may add) is
# ALWAYS executed, followed by running the entry point itself.
#
# See https://wiki.dnanexus.com/Developer-Portal for tutorials on how
# to modify this file.

main() {

    echo "Value of no_of_samples: '$no_of_samples'"

    /cluster/hadoop/bin/hdfs dfsadmin -report
    logPropFile=/cluster/dnax/config/log/log4j-INFO.properties

#    /cluster/spark/bin/spark-submit \
#    --driver-java-options "-Dlog4j.configuration=file:$logPropFile" \
#    --conf "spark.executor.extraJavaOptions=-Dlog4j.configuration=file:$logPropFile" \
#    --class org.apache.spark.examples.SparkPi \
#    --master spark://master:41000 \
#    --deploy-mode client /cluster/spark/examples/jars/spark-examples*.jar $no_of_samples

    #sleep 50000

    python /scripts/dx-spark-submit.py --log-level INFO --collect-log --app-config /scripts/test.json \
        --spark-args '--class org.apache.spark.examples.SparkPi /cluster/spark/examples/jars/spark-examples*.jar 10'
    #python /scripts/dx-spark-submit.py --log-level DEBUG  --collect-log --app-config /scripts/test.json \
    #    --spark-args '--class org.apache.spark.examples.SparkPi /cluster/spark/examples/jars/spark-examples*.jar 10'

}
